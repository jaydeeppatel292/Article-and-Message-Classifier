return (targetLabelsList)
}
print("--------------------------------------------------------")
print("-------------------  NaiveBayes  -----------------------")
predictedLabelsNaiveBayes <- naiveBayes(stackTdmNl[trainData,],tdmTopic[trainData], documentTobeClassified = stackTdmNl[testData,], table(unlist(tdmTopic[trainData])),with_tfidf = FALSE)
NBConfusionMatrix <- confusionMatrix(predictedLabelsNaiveBayes, tdmTopic[testData])
nbAnalysis <- getAnalytics(NBConfusionMatrix)
print(nbAnalysis)
print("-----------------  End NaiveBayes  ---------------------")
print("--------------------------------------------------------")
print("-------------------  KNN Start  -----------------------")
k <- ceiling(sqrt(nrow(stackTdmNl[trainData,])))
knnPred <- kNearestNeighbour(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],k = k  ,with_tfidf = FALSE)
KNNConfusionMatrix <- confusionMatrix(knnPred, tdmTopic[testData])
knnAnalysis <- getAnalytics(KNNConfusionMatrix)
print(knnAnalysis)
print("-------------------  END KNN  -----------------------")
# Initialisation
wd <- getwd()
source(paste(wd,"GenerateTF-IDF.R",sep="/"))
libs <- c("tm", "plyr", "class")
lapply(libs, require, character.only = TRUE)
options(stringsAsFactors = FALSE)
euclidean.dist <- function(x1,x2){
return (sqrt(sum((as.numeric(x1)-as.numeric(x2))^2)))
}
knnConstance <- list("with-tfidf"=TRUE)
kNearestNeighbour <- function(trainTdm,testTdm,tdmTarget,k=10,with_tfidf=TRUE){
knnConstance$"with-tfidf" <- with_tfidf
knnTrainingMatrix <- trainTdm
if(knnConstance$"with-tfidf"){
inverseTermSum = colSums(trainTdm!=0)
knnTrainingMatrix <- generate.tfidf.matrix(trainTdm)
}
prediction.list <- c()
for(testIndex in seq(1:nrow(testTdm))){
testDocToBeClassified <- testTdm[testIndex,]
euclidean.dist.matrix <- data.frame(matrix(ncol = 2,nrow = nrow(knnTrainingMatrix)))
if(knnConstance$"with-tfidf"){
testDocToBeClassified.tfidf <- rep(0,ncol(knnTrainingMatrix))
for(colIndex in seq(1:(ncol(knnTrainingMatrix)))){
termFreq = testDocToBeClassified[colIndex]
if(inverseTermSum[colIndex] == 0){
testDocToBeClassified.tfidf[colIndex] <- 0
}else{
testDocToBeClassified.tfidf[colIndex] <- termFreq * log((nrow(knnTrainingMatrix)/inverseTermSum[colIndex]),10)
}
}
testDocToBeClassified <- testDocToBeClassified.tfidf
}
euclidean.dist.matrix[,1] <- sqrt(rowSums((sweep(knnTrainingMatrix,2,unlist(testDocToBeClassified)))^2))
euclidean.dist.matrix[,2] <- tdmTarget
sorted.euclidean.dist.matrix <- euclidean.dist.matrix[order(euclidean.dist.matrix[,1],decreasing =FALSE),]
prediction.list[testIndex] <-names(which.max(table(sorted.euclidean.dist.matrix[1:k,2])))
}
prediction.list
}
print("--------------------------------------------------------")
print("-------------------  KNN Start  -----------------------")
k <- ceiling(sqrt(nrow(stackTdmNl[trainData,])))
knnPred <- kNearestNeighbour(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],k = k  ,with_tfidf = FALSE)
KNNConfusionMatrix <- confusionMatrix(knnPred, tdmTopic[testData])
knnAnalysis <- getAnalytics(KNNConfusionMatrix)
print(knnAnalysis)
print("-------------------  END KNN  -----------------------")
source("C:\\ConcordiaStudy\\Data Mining\\Project\\Article Classfier New\\Scripts\\GenerateTF-IDF.R")
constants <- list("min_info_gain"=1e-3,"minimum_leaf_size"=3,"with-tfidf"=TRUE)
get_gini_impurity <- function(feature_data){
if(length(feature_data) == 0)
return(0)
return(1- sum((table(feature_data)/length(feature_data))^2))
}
info_gain <- function(y,x,splitValue){
leftSplitedTarget <- y[x<splitValue]
rightSplitedTarget <- y[x>=splitValue]
current_uncertainty <- get_gini_impurity(y)
gini_left_subtree <- get_gini_impurity(leftSplitedTarget)
gini_right_subtree <- get_gini_impurity(rightSplitedTarget)
leftSubTreeSize = length(leftSplitedTarget)
rightSubTreeSize = length(rightSplitedTarget)
totalSize <- length(y)
if ( leftSubTreeSize == 0 || rightSubTreeSize == 0)
return(0)
return (current_uncertainty - (leftSubTreeSize/totalSize)*gini_left_subtree - (rightSubTreeSize/totalSize)*gini_right_subtree)
}
max_information_gain_split <- function(y,x){
best_info_gain = NA
split_value = NA
for( val in sort(unique(x))){
information_gain <- info_gain(y,x,val)
if(is.na(best_info_gain) || information_gain > best_info_gain){
best_info_gain = information_gain
split_value = val
}
}
return(list("best_change"=best_info_gain,
"split_value"=split_value))
}
best_feature_split <- function(X,y){
results <- sapply(X,function(x) max_information_gain_split(y,x))
best_name <- names(which.max(results['best_change',]))
best_result <- results[,best_name]
best_result[["name"]] <- best_name
best_result
}
get_best_mask <- function(X,best_feature_list){
best_mask <- X[,best_feature_list$name] < best_feature_list$split_value
return(best_mask)
}
generateDecisionTree <- function(decisionTreeNode){
best_split <- best_feature_split(decisionTreeNode$dataset,decisionTreeNode$target)
if(best_split$best_change == 0){
return (decisionTreeNode)
}
best_mask <- get_best_mask(decisionTreeNode$dataset,best_split)
decisionTreeNode$best_split <- best_split
leftDf = decisionTreeNode$dataset[best_mask,]
rightDf = decisionTreeNode$dataset[!best_mask,]
if(nrow(leftDf) < constants[["minimum_leaf_size"]] || nrow(rightDf) < constants[["minimum_leaf_size"]]  ){
return(decisionTreeNode)
}
leftDecisionTreeNode <-  list(dataset="data.frame",target=decisionTreeNode$target,best_split=NULL,branches=list("left"=NULL,"right"=NULL))
leftDecisionTreeNode$dataset <- leftDf
leftDecisionTreeNode$target <- decisionTreeNode$target[best_mask]
leftDecisionTreeNode$level <- decisionTreeNode$level +1
rightDecisionTreeNode <-  list(dataset="data.frame",target=decisionTreeNode$target,best_split=NULL,branches=list("left"=NULL,"right"=NULL))
rightDecisionTreeNode$dataset <- rightDf
rightDecisionTreeNode$target <- decisionTreeNode$target[!best_mask]
rightDecisionTreeNode$level <- decisionTreeNode$level +1
decisionTreeNode$branches$left <- leftDecisionTreeNode
decisionTreeNode$branches$left <- generateDecisionTree(decisionTreeNode$branches$left)
decisionTreeNode$branches$right <- rightDecisionTreeNode
decisionTreeNode$branches$right <- generateDecisionTree(decisionTreeNode$branches$right)
decisionTreeNode
}
printDecisionTree <- function(decisionTreeNode){
if(!(is.null(decisionTreeNode))){
if(!(is.null(decisionTreeNode$branches$left))){
noOfSpaces <- paste0("%",(decisionTreeNode$level)*5,"s"," < %s")
print(sprintf(noOfSpaces,decisionTreeNode$best_split$name,formatNumber(decisionTreeNode$best_split$split_value,2)))
printDecisionTree(decisionTreeNode$branches$left)
}
if(!(is.null(decisionTreeNode$branches$right))){
noOfSpaces <- paste0("%",(decisionTreeNode$level)*5,"s"," > %s")
print(sprintf(noOfSpaces,decisionTreeNode$best_split$name,formatNumber(decisionTreeNode$best_split$split_value,2)))
printDecisionTree(decisionTreeNode$branches$right)
}
if(is.null(decisionTreeNode$branches$left) && is.null(decisionTreeNode$branches$right)){
noOfSpaces <- paste0("%",(decisionTreeNode$level)*6,"s","%s")
print(sprintf(noOfSpaces,"Decision ----->",names(which.max(table(decisionTreeNode$target)))))
}
}
}
predict_data = function(row,decisioTreeNode){
if(is.null(decisioTreeNode$branches$left)){
predict_value <-  names(which.max(table(decisioTreeNode$target)))
} else {
if(row[decisioTreeNode$best_split$name] < decisioTreeNode$best_split$split_value){
predict_value = predict_data(row,decisioTreeNode$branches$left)
} else {
predict_value = predict_data(row,decisioTreeNode$branches$right)
}
}
return(predict_value)
}
predict_test_dataset <- function(testDataset,decisionTreeRootNode){
predict_list <- list()
inverseTermSum = NULL
if(constants[["with-tfidf"]]){
inverseTermSum = colSums(testDataset!=0)
}
for(testIndex in 1:nrow(testDataset)){
testData <- testDataset[testIndex,]
if(constants[["with-tfidf"]]){
testData <- rep(0,ncol(decisionTreeRootNode$dataset))
for(colIndex in (1:(ncol(decisionTreeRootNode$dataset)))){
termFreq = testDataset[testIndex,colIndex]
if(inverseTermSum[colIndex] == 0){
testData[colIndex] <- 0
}else{
testData[colIndex] <- termFreq * log((nrow(decisionTreeRootNode$dataset)/inverseTermSum[colIndex]),10)
}
}
}
predict_list <- c(predict_list,predict_data(testDataset[testIndex,],decisionTreeRootNode))
}
predict_list
}
decisionTree <- function(trainDataset,trainTarget,testDataset,with_tfidf=TRUE){
constants[["with-tfidf"]] <- with_tfidf
if(constants[["with-tfidf"]]){
trainDataset <-  generate.tfidf.matrix(trainDataset)
}
DecisionTreeRootNode <- list(dataset="data.frame",target=c(),best_split=NULL,branches=list("left"=NULL,"right"=NULL),level=2)
DecisionTreeRootNode$dataset <- trainDataset
DecisionTreeRootNode$target <- trainTarget
DecisionTreeRootNode <- generateDecisionTree(DecisionTreeRootNode)
printDecisionTree(DecisionTreeRootNode)
predict_test_dataset(testDataset,DecisionTreeRootNode)
}
formatNumber <- function(number,decimalPoints){
format(round(number, decimalPoints), nsmall = decimalPoints)
}
print("--------------------------------------------------------")
print("-------------------  Decision Tree  -----------------------")
decisionTree_result <- decisionTree(stackTdmNl[trainData,], tdmTopic[trainData], stackTdmNl[testData,],with_tfidf = FALSE)
decisionTreeConfusionMatrix <- confusionMatrix(unlist(decisionTree_result),tdmTopic[testData])
decionTreeAnalysis <- getAnalytics(decisionTreeConfusionMatrix)
print(decionTreeAnalysis)
print("------------------- END Decision Tree  -----------------------")
print("--------------------------------------------------------")
print("-------------------  SVM Start  -----------------------")
y_pred <- svm.train(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],kernal = linear.kernal) # specify diffrent kernal from {linear.kernal,gaussian.kernal,poly.kernal,rbf.kernal}
svmConfusionMatrix <- confusionMatrix(y_pred, tdmTopic[testData])
svmAnalysis <- getAnalytics(svmConfusionMatrix)
print(svmAnalysis)
print("-------------------  END SVM  -----------------------")
library(quadprog)
dot <- function(x1,x2){
(x1)%*%(x2)
}
linear.kernal <- function(x1,x2){
dot(x1,x2)
}
rbf.kernal <- function(x1, x2){
gamma = 0.7
(exp(-gamma * norm(x1 - x2,"2") ** 2))
}
polynomial.kernal <- function(x1,x2,p=3){
((1+ dot(x1,x2))**p)
}
gaussian.kernal <- function(x1,x2,sigma=5.0){
(exp(-norm(x1-x2,"2")**2/(2*(sigma**2))))
}
center = function(z) (z-mean(z))/sd(z)
svm.fit = function(X, y, FUN=linear.kernal, C=NULL) {
n.samples = nrow(X)
n.features = ncol(X)
K = matrix(rep(0, n.samples*n.samples), nrow=n.samples)
for (i in 1:n.samples){
for (j in 1:n.samples){
K[i,j] = FUN(X[i,], X[j,])
}
}
Dmat = outer(y,y) * K
Dmat = as.matrix(Matrix::nearPD(Dmat)$mat)
dvec = rep(1, n.samples)
Amat = rbind(y, diag(n.samples), -1*diag(n.samples))
bvec = c(0, rep(0, n.samples), rep(-C, n.samples))
res = solve.QP(Dmat,dvec,t(Amat),bvec=bvec, meq=1)
a = res$unconstrained
bomega = apply(a*y*X,2,sum)
return(bomega)
}
svm.train <- function(trainTdm,testTdm,tdmTarget,kernal=linear.kernal){
svm.multiclass = c()
class_list <- unique(tdmTarget)
rootSVM <- list("bomega"=NULL,"class"=NULL,"child"=NULL)
trainTDMData <- as.matrix(trainTdm)
colnames(trainTDMData) <- NULL
testTDMData <- as.matrix(testTdm)
colnames(testTDMData) <- NULL
svm.matrix <- list()
matrix.index=1
for(i in 1:(length(class_list)-1)){
print("Wait for a while to complete svm fitting process")
for(j in (i+1):length(class_list)){
class1 <- class_list[i]
class2 <- class_list[j]
topic <- tdmTarget[tdmTarget%in%c(class1,class2)]
topic[topic==class1] = 1
topic[topic!=1] = -1
dataset.svm <- trainTDMData[tdmTarget%in%c(class1,class2),]
bomega <- svm.fit(cbind(1,dataset.svm),as.numeric(topic),kernal,0.5)
svm.matrix[[matrix.index]] <- list("from","to","bomega")
svm.matrix[[matrix.index]]$from <- class1
svm.matrix[[matrix.index]]$to <- class2
svm.matrix[[matrix.index]]$bomega <- bomega
matrix.index = matrix.index+1
}
}
y_pred <- c()
for(testDataIndex in 1:nrow(testTDMData)){
test.Data <- testTDMData[testDataIndex,]
predicted <- c()
for(svmclassindex in 1:length(svm.matrix)){
from <- svm.matrix[[svmclassindex]]$from
to <- svm.matrix[[svmclassindex]]$to
bomega <- svm.matrix[[svmclassindex]]$bomega
if((2*((c(1,test.Data) %*% bomega)>0)-1)==1){
predict <- from
}else{
predict <- to
}
if(is.null(predicted[predict])|| is.na(predicted[predict])){
predicted[predict] <-0
}
predicted[predict]<- as.numeric(predicted[predict])+1
}
y_pred[testDataIndex] =names(predicted[predicted==max(predicted)])[1]
}
return(y_pred)
}
print("--------------------------------------------------------")
print("-------------------  SVM Start  -----------------------")
y_pred <- svm.train(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],kernal = rbf.kernal) # specify diffrent kernal from {linear.kernal,gaussian.kernal,polynomial.kernal,rbf.kernal}
svmConfusionMatrix <- confusionMatrix(y_pred, tdmTopic[testData])
svmAnalysis <- getAnalytics(svmConfusionMatrix)
print(svmAnalysis)
print("-------------------  END SVM  -----------------------")
libs <- c("quadprog")
lapply(libs, require, character.only = TRUE)
dot <- function(x1,x2){
(x1)%*%(x2)
}
linear.kernal <- function(x1,x2){
dot(x1,x2)
}
rbf.kernal <- function(x1, x2){
gamma = 0.7
(exp(-gamma * norm(x1 - x2,"2") ** 2))
}
polynomial.kernal <- function(x1,x2,p=3){
((1+ dot(x1,x2))**p)
}
gaussian.kernal <- function(x1,x2,sigma=5.0){
(exp(-norm(x1-x2,"2")**2/(2*(sigma**2))))
}
center = function(z) (z-mean(z))/sd(z)
svm.fit = function(X, y, FUN=linear.kernal, C=NULL) {
n.samples = nrow(X)
n.features = ncol(X)
K = matrix(rep(0, n.samples*n.samples), nrow=n.samples)
for (i in 1:n.samples){
for (j in 1:n.samples){
K[i,j] = FUN(X[i,], X[j,])
}
}
Dmat = outer(y,y) * K
Dmat = as.matrix(Matrix::nearPD(Dmat)$mat)
dvec = rep(1, n.samples)
Amat = rbind(y, diag(n.samples), -1*diag(n.samples))
bvec = c(0, rep(0, n.samples), rep(-C, n.samples))
res = solve.QP(Dmat,dvec,t(Amat),bvec=bvec, meq=1)
a = res$unconstrained
bomega = apply(a*y*X,2,sum)
return(bomega)
}
svm.train <- function(trainTdm,testTdm,tdmTarget,kernal=linear.kernal){
svm.multiclass = c()
class_list <- unique(tdmTarget)
rootSVM <- list("bomega"=NULL,"class"=NULL,"child"=NULL)
trainTDMData <- as.matrix(trainTdm)
colnames(trainTDMData) <- NULL
testTDMData <- as.matrix(testTdm)
colnames(testTDMData) <- NULL
svm.matrix <- list()
matrix.index=1
for(i in 1:(length(class_list)-1)){
print("Wait for a while to complete svm fitting process")
for(j in (i+1):length(class_list)){
class1 <- class_list[i]
class2 <- class_list[j]
topic <- tdmTarget[tdmTarget%in%c(class1,class2)]
topic[topic==class1] = 1
topic[topic!=1] = -1
dataset.svm <- trainTDMData[tdmTarget%in%c(class1,class2),]
bomega <- svm.fit(cbind(1,dataset.svm),as.numeric(topic),kernal,0.5)
svm.matrix[[matrix.index]] <- list("from","to","bomega")
svm.matrix[[matrix.index]]$from <- class1
svm.matrix[[matrix.index]]$to <- class2
svm.matrix[[matrix.index]]$bomega <- bomega
matrix.index = matrix.index+1
}
}
y_pred <- c()
for(testDataIndex in 1:nrow(testTDMData)){
test.Data <- testTDMData[testDataIndex,]
predicted <- c()
for(svmclassindex in 1:length(svm.matrix)){
from <- svm.matrix[[svmclassindex]]$from
to <- svm.matrix[[svmclassindex]]$to
bomega <- svm.matrix[[svmclassindex]]$bomega
if((2*((c(1,test.Data) %*% bomega)>0)-1)==1){
predict <- from
}else{
predict <- to
}
if(is.null(predicted[predict])|| is.na(predicted[predict])){
predicted[predict] <-0
}
predicted[predict]<- as.numeric(predicted[predict])+1
}
y_pred[testDataIndex] =names(predicted[predicted==max(predicted)])[1]
}
return(y_pred)
}
install.packages(c("quadprog","tm", "plyr", "class"))
install.packages("quadprog","tm", "plyr", "class")
# Initialization
wd <- getwd()
source(paste(wd,"GenerateTDM.R",sep="/"))
source(paste(wd,"GenerateTF-IDF.R",sep="/"))
source(paste(wd,"FeatureCategoryMatrix.R",sep="/"))
source(paste(wd,"Naive_Bayes.R",sep="/"))
source(paste(wd,"ConfusionMatrix.R",sep="/"))
source(paste(wd,"K-NearestNeighbours.R",sep="/"))
source(paste(wd,"decision_tree.R",sep="/"))
libs <- c("tm", "plyr", "class")
is.nan.data.frame <- function(x){
do.call(cbind,lapply(x,is.nan))
}
lapply(libs, require, character.only = TRUE)
options(stringsAsFactors = FALSE)
#topics <- c("ITD.csv", "OAA.csv", "OSL.csv")
topics <- c("business", "entertainment", "politics", "sport", "tech")
#Replace your path to data directory here
#pathname <- paste(wd,"/../Dataset/University_Dataset")
pathname <- paste(wd,"/../Dataset/Article_Data")
# Clean DataSets
removeSparseTermsFromTdm <- function(tdm,sparsity){
tdm[,!colSums(tdm!=0)<(1-sparsity)*nrow(tdm)]
}
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, tolower)
corpus.tmp <- tm_map(corpus.tmp, removeWords, stopwords("english"))
corpus.tmp <- tm_map(corpus.tmp,removeWords,c("ad","us","2003","2005","mr","2004","uk","go","next","im"))
corpus.tmp <- tm_map(corpus.tmp, stemDocument)
return (corpus.tmp)
}
# Generate Term Document Matrix
generateTermDocumentMatrix <- function(topic, pathname){
directory <- sprintf("%s\\%s", pathname, topic)
#dataframe <- read.csv(directory,stringsAsFactors=F)
corpus <- Corpus(DirSource(directory = directory, encoding ="UTF-8"))
#corpus <- Corpus(DataframeSource(dataframe))
corpus_clean <- cleanCorpus(corpus)
tdm <- generate.tdm(corpus_clean)
tdm <- removeSparseTermsFromTdm(tdm,0.8)
result <- list(topic = topic, tdm = tdm)
}
tdm <- lapply(topics, generateTermDocumentMatrix, pathname = pathname)
# Bind Topic to Term Document Matrix
bindTopicToTermDocumentMatrix <- function(tdm){
dataframe <-  (tdm[["tdm"]])
dataframe <- cbind(dataframe, rep(tdm[["topic"]], nrow(dataframe)))
colnames(dataframe)[ncol(dataframe)] <- "targetTopic"
return (dataframe)
}
topicTDM <- lapply(tdm, bindTopicToTermDocumentMatrix)
# Stack all Datasets
stackTDM <- do.call(rbind.fill, topicTDM)
stackTDM[is.na(stackTDM)] <- 0
# Remove extra columns
stackTDM <- stackTDM[,!colnames(stackTDM) %in% c("ad","us","2003","2005","mr","2004","uk","go","next","im")]
# Split target topic and TDM
tdmTopic <- stackTDM[,"targetTopic"]
stackTdmNl  <- stackTDM[,!colnames(stackTDM) %in% "targetTopic"]
# Split training and test data
trainData <- sample(nrow(stackTDM),ceiling(nrow(stackTDM)*0.8))
testData <- (1:nrow(stackTDM))[- trainData]
source(paste(wd,"GenerateTDM.R",sep="/"))
source(paste(wd,"GenerateTF-IDF.R",sep="/"))
source(paste(wd,"FeatureCategoryMatrix.R",sep="/"))
source(paste(wd,"Naive_Bayes.R",sep="/"))
source(paste(wd,"ConfusionMatrix.R",sep="/"))
source(paste(wd,"K-NearestNeighbours.R",sep="/"))
source(paste(wd,"analysis.R",sep="/"))
source(paste(wd,"decision_tree.R",sep="/"))
source(paste(wd,"svm.R",sep="/"))
print("--------------------------------------------------------")
print("-------------------  NaiveBayes  -----------------------")
predictedLabelsNaiveBayes <- naiveBayes(stackTdmNl[trainData,],tdmTopic[trainData], documentTobeClassified = stackTdmNl[testData,], table(unlist(tdmTopic[trainData])),with_tfidf = FALSE)
NBConfusionMatrix <- confusionMatrix(predictedLabelsNaiveBayes, tdmTopic[testData])
nbAnalysis <- getAnalytics(NBConfusionMatrix)
print(NBConfusionMatrix)
print(nbAnalysis)
print("-----------------  End NaiveBayes  ---------------------")
decionTreeAnalysis
print("--------------------------------------------------------")
print("-------------------  KNN Start  -----------------------")
k <- ceiling(sqrt(nrow(stackTdmNl[trainData,])))
knnPred <- kNearestNeighbour(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],k = k  ,with_tfidf = FALSE)
KNNConfusionMatrix <- confusionMatrix(knnPred, tdmTopic[testData])
knnAnalysis <- getAnalytics(KNNConfusionMatrix)
print(KNNConfusionMatrix)
print(knnAnalysis)
print("-------------------  END KNN  -----------------------")
print("--------------------------------------------------------")
print("-------------------  Decision Tree  -----------------------")
decisionTree_result <- decisionTree(stackTdmNl[trainData,], tdmTopic[trainData], stackTdmNl[testData,],with_tfidf = FALSE)
decisionTreeConfusionMatrix <- confusionMatrix(unlist(decisionTree_result),tdmTopic[testData])
decionTreeAnalysis <- getAnalytics(decisionTreeConfusionMatrix)
print(decisionTreeConfusionMatrix)
print(decionTreeAnalysis)
print("------------------- END Decision Tree  -----------------------")
print("--------------------------------------------------------")
print("-------------------  SVM Start  -----------------------")
y_pred <- svm.train(stackTdmNl[trainData,], stackTdmNl[testData,], tdmTopic[trainData],kernal = rbf.kernal) # specify diffrent kernal from {linear.kernal,gaussian.kernal,polynomial.kernal,rbf.kernal}
svmConfusionMatrix <- confusionMatrix(y_pred, tdmTopic[testData])
svmAnalysis <- getAnalytics(svmConfusionMatrix)
print(svmConfusionMatrix)
print(svmAnalysis)
print("-------------------  END SVM  -----------------------")
# Initialization
list.of.packages <- c("tm", "plyr", "class","quadprog")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
wd <- getwd()
wd
